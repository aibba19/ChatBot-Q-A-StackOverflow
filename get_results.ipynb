{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87554b71-30d9-46ae-8601-bbdff479dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import spacy\n",
    "%run create_embeddings.ipynb\n",
    "import preprocessing\n",
    "import tfidf\n",
    "import scipy\n",
    "pd.set_option('max_colwidth', 5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ed60d8-463a-4b47-9f00-8b8ff5c28e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importazione DB\n",
    "\n",
    "#Data on Stack Overflow DB already preprocessed\n",
    "data_stack = pd.read_csv('DB/Preprocessed_data.csv')\n",
    "\n",
    "#data = data[data['processed_title'].notna()]\n",
    "\n",
    "#Data on Ontology DB already preprocessed\n",
    "data_onto = pd.read_csv('DB/Preprocessed_ontology.csv')\n",
    "\n",
    "#Matrix for stack tfidf processing \n",
    "#tfidf_stack = scipy.sparse.load_npz('DB/tfidf_stack_matrix.npz')\n",
    "stack_corpus = data_stack['processed_title'] + data_stack['questions_processed'] + data_stack['answers_processed'] \n",
    "stack_corpus = stack_corpus.dropna()\n",
    "\n",
    "#Matrix for ontology tfidf processing\n",
    "#tfidf_onto = scipy.sparse.load_npz('DB/tfidf_onto_matrix.npz')\n",
    "onto_corpus = data_onto.description_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497782e1-f9ce-4ec9-a3c1-5505b070c1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x2953aa60c10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importazione WordEmbeddings created in file 'train_word2vec'\n",
    "\n",
    "word_2_vec_model = gensim.models.word2vec.Word2Vec.load('DB/word2vec_trained.bin')\n",
    "word_2_vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae1e41e-af12-4907-aae0-82a557d661e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fucntion to clear raw txt \n",
    "def normalize(txt):\n",
    "    phrase = preprocessing.clear_text(txt)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02874628-8efd-4f71-855b-b3f05f95023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fucntion to calculate cosine similarity between user raw txt and stack overflow processed title with word to vec  \n",
    "def w2v_stack_result(txt):\n",
    "    stringSearch = normalize(txt)\n",
    "\n",
    "    #print(stringSearch)\n",
    "\n",
    "    numberResult = 5\n",
    "\n",
    "    #Viene vettorizzata la query dell'utente\n",
    "\n",
    "    vectorSearch = np.array([questionToVec(stringSearch, word_2_vec_model)])\n",
    "    \n",
    "    #Carico il file con gli embeddings dei titoli sui quali effettuare la corrispondenza\n",
    "    allTitleEmb = pd.read_csv('DB/titleEmbeddings.csv').values\n",
    "\n",
    "    #Calcolo della similarità del coseno for le query e tutti i titoli\n",
    "    similarityCosine = pd.Series(cosine_similarity(vectorSearch, allTitleEmb)[0])\n",
    "    \n",
    "    #Personalizzazione della misura di similarità\n",
    "    similarityCosine = similarityCosine*(1+0.4*data_stack.score)\n",
    "    \n",
    "    #Lista che conterrà coppie i,j con i indice e j score di similarità\n",
    "    results = []\n",
    "    \n",
    "    #Una volta calcolati gli score di similarità rendo i 10 più alti\n",
    "    for i,similiarity_score in similarityCosine.nlargest(numberResult).iteritems():\n",
    "        #print(i, data.Body[i])\n",
    "        #Filtro i tag da rendere come risultato, eliminando quelli che sono già presenti nella string search per facilitare prossimi passaggi \n",
    "        tags = data_stack.iloc[[i]].tags.item().split('|')\n",
    "        #print(tags)\n",
    "        for t in tags:\n",
    "            if t in stringSearch:\n",
    "                tags.remove(t)\n",
    "                \n",
    "        stack_id = data_stack.iloc[[i]].id.values[0]\n",
    "        stack_title = data_stack.iloc[[i]].title.values[0]\n",
    "        \n",
    "        results.append(tuple([stack_id, similiarity_score, tags, stack_title ]))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd3ed64-f52a-455b-8c44-1ffddf9ff6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_ontology_result(txt):\n",
    "    stringSearch = normalize(txt)\n",
    "\n",
    "    #print(stringSearch)\n",
    "\n",
    "    numberResult = 3\n",
    "\n",
    "    #Viene vettorizzata la query dell'utente\n",
    "\n",
    "    vectorSearch = np.array([questionToVec(stringSearch, word_2_vec_model)])\n",
    "    \n",
    "    #Carico il file con gli embeddings dei titoli sui quali effettuare la corrispondenza\n",
    "    allFuncEmb = pd.read_csv('DB/functionEmbeddings.csv').values\n",
    "\n",
    "    #Calcolo della similarità del coseno for le query e tutti i titoli\n",
    "\n",
    "    similarityCosine = pd.Series(cosine_similarity(vectorSearch, allFuncEmb)[0])\n",
    "    \n",
    "    #Lista che conterrà coppie i,j con i indice e j score di similarità\n",
    "    results = []\n",
    "    \n",
    "      \n",
    "    #Una volta calcolati gli score di similarità rendo i 10 più alti\n",
    "    for i,similiarity_score in similarityCosine.nlargest(numberResult).iteritems():\n",
    "        #print(i, data.Body[i])\n",
    "        #Filtro i tag da rendere come risultato, eliminando quelli che sono già presenti nella string search per facilitare prossimi passaggi \n",
    "        description = data_onto.iloc[[i]].description.item()\n",
    "        function = data_onto.iloc[[i]].function.item()\n",
    "                    \n",
    "        results.append(tuple([function,description, similiarity_score]))\n",
    "      \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "587d17bb-4eed-41c7-b23e-bcd7c9cee857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_stack_result(txt):\n",
    "    stringSearch = normalize(txt)\n",
    "    related_docs_indices = tfidf.get_results(stack_corpus, stringSearch)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx in related_docs_indices:\n",
    "        \n",
    "        stack_id = data_stack.iloc[[idx]].id.values[0]\n",
    "        stack_title = data_stack.iloc[[idx]].title.values[0]\n",
    "        tags = data_stack.iloc[[idx]].tags.item().split('|')\n",
    "        \n",
    "        results.append(tuple([stack_id, 0, tags, stack_title ]))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b773d87-4eb7-436e-aa2e-208823e1b6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9321 37621 21074 13685]\n"
     ]
    }
   ],
   "source": [
    "indices = tfidf_stack_result('how to delete nan values from series')\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "323d5df9-e24c-4c51-b116-7406faf7c7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13685    Putting lower bound and upper bounds on numpy.random.exponential\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stack.loc[[13685]].title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c567f95-0456-40ea-a1cd-324e6df6d59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(45940141,\n",
       "  0.9177789336288047,\n",
       "  ['for-loop'],\n",
       "  'How would I change this one line for loop to normal for loop?'),\n",
       " (45812581,\n",
       "  0.9134695274094327,\n",
       "  ['pandas', 'scikit-learn', 'feature-extraction'],\n",
       "  'Replicated rows as dictionary in pandas for feature extraction'),\n",
       " (65957446,\n",
       "  0.9024597896899457,\n",
       "  ['string', 'for-loop', 'readlines'],\n",
       "  'IndexError: list index out of range in a loop of readlines()'),\n",
       " (33485294,\n",
       "  0.9014093577320642,\n",
       "  ['django', 'heroku', 'gunicorn', 'mezzanine'],\n",
       "  \"Mezzanine - Can't load css and js in Heroku\"),\n",
       " (45807268,\n",
       "  0.8989503109464376,\n",
       "  ['javascript', 'json'],\n",
       "  'Issue with parsing (via JavaScript) an automatically created (via Python) JSON file')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_stack_result('join column of two datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0b2fb-5c03-481f-aaaf-72f88d86efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import joblib\n",
    "#import scipy.sparse\n",
    "#import tfidf\n",
    "\n",
    "#matr = scipy.sparse.load_npz('DB/wc_matrix.npz')\n",
    "#cvec = joblib.load(\"DB/count_vec.pkl\")\n",
    "\n",
    "#out = tfidf.tf_idf(matr,cvec,'how to delete nan values from series')\n",
    "#print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28d370-2d6e-42b6-bcef-83eb98edde82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
